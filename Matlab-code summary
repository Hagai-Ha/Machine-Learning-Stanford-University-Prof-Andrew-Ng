################################   Week 2:   ################################
Linear regression

Hypothesis function:
h(X, theta) = X * theta

Cost function:
J(theta) = J = ((h-y)'*(h-y))/(2*m)

Gradient Descent:
theta = theta - (alpha/m) * X' * (h - y)

Normal Equation:    (more efficient than Gradient Descent if the number of features is n<10,000)
θ=pinv(X'X)X'y

################################   Week 3:   ################################

Logistic regression (sigmoid function)

Logistic Regression Hypothesis function:
h(X, theta) = 1 / ( 1 + e^(-theta' * X)  )

Decision boundary:
theta' * X = 0
y = 1 if theta' * X >= 0
y = 0 if theta' * X < 0

Logistic Regression Cost function:
J = (1/m) * sum(Cost(h,y))
if y = 1 --> Cost = -log(h)
if y = 0 --> Cost = -log(1-h)
    |
    |
    V
Cost = -y*log(h) -(1-y)log(1-h)
J = (1/m) * ( -y'*log(h) -(1-y)'*log(1-h) )

Gradient Descent:
theta = theta -(alpha/m)*X'*(h-y)

# Examples of more advanced and a lot more complexed algorithms which could be used instead of Gradient Descent:
# "Conjugate gradient"
# "BFGS"
# "L-BFGS"
# All the said algorithms choose the learning rate "alpha" automatically, and often compute theta faster. They are usually learned in courses of advanced numerical computing methods. They could be implemented easily using certain libraries (Tip: try out different libraries, since some have better implementations than others...)

# Multiclass classification: One-Vs-All


# Underfitting = high bias
# Overfitting = high variance (for example due to using a too high polynomial order while choosing the features)
# Both do not generalize well, and fail to predict "new" data.

# Addressing overfitting:
# 1. Reduce the number of features:
#       - manually or automatically choosing and keeping the most important features.
# 2. Regularization:
#       - reducing the magnitude of the values of theta. This method is favorable if there are a lot of features, and each of them contributes equally important to the prediction.

# Regularization

Regularized Linear Regression:

# delta = d(theta)/d(J) = "the gradient"

Cost function:
J(theta) = J = (1/(2*m))*( (h-y)'*(h-y) + lambda*theta'*theta )

Gradient Descent:
theta0 = theta0 -(alpha/m)*X0'*(h-y)
theta = theta -alpha*delta
theta = theta*(1-(alpha*lambda)/m) - (alpha/m) * X' * (h - y)             # for theta 1...n (we don't regularize theta0)

Normal Equation:    
θ=pinv(X'X + lambda*L)X'y
    where L = [] an (n+1 x n+1) Identity matrix, with the first element switched to 0. i.e. I[0,0] = 0
    Regularizing the Normal Equation also helps with the uninvertibility problem.

Regularized Logistic Regression:

z = X*theta; % (m x 1) = (m x n+1) * (n+1 x 1)
h = sigmoid(z); % (m x 1)

Cost function:
J = (1/m) * ( -y'*log(h) -(1-y)'*log(1-h) ) + (lambda/(2*m)) * theta(2:length(theta))'*theta(2:length(theta))

delta(j) = gradient(j) = (1/m) * x(j) * (h-y) + (lambda/m)*theta(j)         # For all j > 0
grad = (1/m) * X'*(h-y); % (n+1 x 1)
grad(2:length(grad)) = grad(2:length(grad)) + (lambda/m)*theta(2:length(grad)); % adding Reg to all theta except theta0 (theta(1))

################################   Week 4: Multi-class logistic regression classifiers & Neural Networks   ################################

% MATLAB built-in optimization functions:
%       fmincg works similarly to fminunc, but is more efficient when we
%       are dealing with large number of parameters.





