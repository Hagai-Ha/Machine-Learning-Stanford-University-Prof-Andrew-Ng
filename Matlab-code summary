################################   Week 2:   ################################
Linear regression

Hypothesis function:
h(X, theta) = X * theta

Cost function:
J(theta) = J = ((h-y)'*(h-y))/(2*m)

Gradient Descent:
theta = theta - (alpha/m) * X' * (h - y)

Normal Equation:    (more efficient than Gradient Descent if the number of features is n<10,000)
θ=pinv(X'X)X'y

################################   Week 3:   ################################

Logistic regression (sigmoid function)

Logistic Regression Hypothesis function:
h(X, theta) = 1 / ( 1 + e^(-theta' * X)  )

Decision boundary:
theta' * X = 0
y = 1 if theta' * X >= 0
y = 0 if theta' * X < 0

Logistic Regression Cost function:
J = (1/m) * sum(Cost(h,y))
if y = 1 --> Cost = -log(h)
if y = 0 --> Cost = -log(1-h)
    |
    |
    V
Cost = -y*log(h) -(1-y)log(1-h)
J = (1/m) * ( -y'*log(h) -(1-y)'*log(1-h) )

Gradient Descent:
theta = theta -(alpha/m)*X'*(h-y)

# Examples of more advanced and a lot more complexed algorithms which could be used instead of Gradient Descent:
# "Conjugate gradient"
# "BFGS"
# "L-BFGS"
# All the said algorithms choose the learning rate "alpha" automatically, and often compute theta faster. They are usually learned in courses of advanced numerical computing methods. They could be implemented easily using certain libraries (Tip: try out different libraries, since some have better implementations than others...)

# Multiclass classification: One-Vs-All


# Underfitting = high bias
# Overfitting = high variance (for example due to using a too high polynomial order while choosing the features)
# Both do not generalize well, and fail to predict "new" data.

# Addressing overfitting:
# 1. Reduce the number of features:
#       - manually or automatically choosing and keeping the most important features.
# 2. Regularization:
#       - reducing the magnitude of the values of theta. This method is favorable if there are a lot of features, and each of them contributes equally important to the prediction.

# Regularization

Regularized Linear Regression:

Cost function:
J(theta) = J = (1/(2*m))*( (h-y)'*(h-y) + lambda*sum(theta^2) )

Gradient Descent:
theta = theta*(1-(alpha*lambda)/m) - (alpha/m) * X' * (h - y)

Normal Equation:    
θ=pinv(X'X + lambda*L)X'y
    where L = [] an (n+1 x n+1) Identity matrix, with the first element switched to 0. i.e. I[0,0] = 0
